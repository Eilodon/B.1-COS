//! Advanced Active Inference Planning Tests
//!
//! This module contains comprehensive tests for the Active Inference planning system,
//! including multi-step planning scenarios and non-obvious solution paths.

#[cfg(test)]
use crate::{
    ActiveInferenceSankharaSkandha, LearningEngine, NeuralQValueEstimator, ValueDrivenPolicy,
};
#[cfg(test)]
use bytes::Bytes;
#[cfg(test)]
use pandora_core::interfaces::skandhas::SankharaSkandha;
#[cfg(test)]
use pandora_core::ontology::EpistemologicalFlow;
#[cfg(test)]
use pandora_cwm::gnn::types::GnnConfig;
#[cfg(test)]
use pandora_cwm::model::InterdependentCausalModel;
#[cfg(test)]
use std::sync::{Arc, Mutex};

/// Test multi-step planning with non-obvious solution
/// Scenario: Pick up key in room A to open box in room C, ignoring distracting item in room B
#[cfg(test)]
#[test]
fn test_multi_step_planning_complex_scenario() {
    println!("üß† Testing multi-step planning: Key in A ‚Üí Box in C (ignore B)");

    // Create a more complex CWM for multi-room scenario
    let config = GnnConfig::new(128, 256, 5);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.8, 0.2));

    // Define actions for the multi-room scenario
    let available_actions = vec![
        "move_to_room_a",
        "move_to_room_b",
        "move_to_room_c",
        "pick_up_key",
        "pick_up_distractor",
        "open_box",
        "check_room_contents",
        "examine_item",
        "use_key",
        "wait",
        "explore",
        "plan_path",
    ];

    // Create value-driven policy with longer planning horizon
    let _q_estimator = NeuralQValueEstimator::new(0.1, 0.95);
    let _policy = ValueDrivenPolicy::new(0.1, 0.95, 0.2);

    // Create ActiveInferenceSankharaSkandha with extended planning
    let sankhara = ActiveInferenceSankharaSkandha::new(
        cwm.clone(),
        learning_engine.clone(),
        8, // Extended planning horizon for multi-step scenarios
        available_actions,
        0.95, // High gamma for long-term planning
        0.15, // Moderate exploration
    );

    // Simulate the complex scenario
    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"multi_room_scenario"));

    // Set initial state: Agent in starting position, goal is to open box in room C
    flow.sankhara = Some(std::sync::Arc::from("plan_path"));

    println!("üéØ Scenario: Agent needs to get key from room A to open box in room C");
    println!("   Distraction: There's an attractive item in room B that should be ignored");
    println!("   Optimal path: Start ‚Üí Room A (get key) ‚Üí Room C (open box)");
    println!("   Suboptimal path: Start ‚Üí Room B (distraction) ‚Üí Room C (fail)");

    // Test the planning system
    let mut planning_steps = Vec::new();
    let mut current_room = "start";
    let mut has_key = false;
    let mut box_opened = false;

    // Simulate planning and execution
    for step in 1..=10 {
        println!(
            "\nüîÑ Planning step {}: Current room = {}, Has key = {}, Box opened = {}",
            step, current_room, has_key, box_opened
        );

        // Let the SankharaSkandha form an intent (plan next action)
        sankhara.form_intent(&mut flow);

        let planned_action = flow.sankhara.as_ref().map(|s| s.as_ref()).unwrap_or("wait");

        println!("   Planned action: {}", planned_action);
        planning_steps.push(planned_action.to_string());

        // Simulate action execution and state transition
        match planned_action {
            "move_to_room_a" => {
                current_room = "room_a";
                println!("   ‚úÖ Moved to room A");
            }
            "move_to_room_b" => {
                current_room = "room_b";
                println!("   ‚ö†Ô∏è  Moved to room B (distraction room)");
            }
            "move_to_room_c" => {
                current_room = "room_c";
                println!("   ‚úÖ Moved to room C");
            }
            "pick_up_key" => {
                if current_room == "room_a" {
                    has_key = true;
                    println!("   ‚úÖ Picked up key from room A");
                } else {
                    println!("   ‚ùå No key in this room");
                }
            }
            "pick_up_distractor" => {
                if current_room == "room_b" {
                    println!("   ‚ö†Ô∏è  Picked up distractor item (suboptimal)");
                } else {
                    println!("   ‚ùå No distractor in this room");
                }
            }
            "open_box" => {
                if current_room == "room_c" && has_key {
                    box_opened = true;
                    println!("   üéâ Successfully opened box in room C!");
                } else if current_room == "room_c" && !has_key {
                    println!("   ‚ùå Cannot open box without key");
                } else {
                    println!("   ‚ùå No box in this room");
                }
            }
            "check_room_contents" => match current_room {
                "room_a" => println!("   üîç Room A contains: key"),
                "room_b" => println!("   üîç Room B contains: distractor item"),
                "room_c" => println!("   üîç Room C contains: locked box"),
                _ => println!("   üîç Empty room"),
            },
            _ => {
                println!("   ‚è∏Ô∏è  Other action: {}", planned_action);
            }
        }

        // Check if goal is achieved
        if box_opened {
            println!("   üéØ GOAL ACHIEVED! Box opened successfully");
            break;
        }

        // Check for suboptimal behavior
        if step > 5 && current_room == "room_b" {
            println!("   ‚ö†Ô∏è  WARNING: Agent seems stuck in distraction room B");
        }
    }

    // Analyze the planning performance
    println!("\nüìä Planning Analysis:");
    println!("   Total steps: {}", planning_steps.len());
    println!("   Actions taken: {:?}", planning_steps);
    println!(
        "   Final state: Room = {}, Has key = {}, Box opened = {}",
        current_room, has_key, box_opened
    );

    // Verify the planning worked correctly
    assert!(box_opened, "Agent should have successfully opened the box");
    assert!(has_key, "Agent should have picked up the key");
    assert_eq!(current_room, "room_c", "Agent should end up in room C");

    // Check for optimal path (should visit room A before room C)
    let room_a_visited = planning_steps.contains(&"move_to_room_a".to_string());
    let key_picked = planning_steps.contains(&"pick_up_key".to_string());

    assert!(room_a_visited, "Agent should have visited room A");
    assert!(key_picked, "Agent should have picked up the key");

    println!("‚úÖ Multi-step planning test passed!");
    println!("   Agent successfully planned and executed optimal path");
}

/// Test planning with dynamic obstacles and alternative paths
#[cfg(test)]
#[test]
fn test_dynamic_obstacle_planning() {
    println!("üöß Testing planning with dynamic obstacles");

    let config = GnnConfig::new(96, 192, 4);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.75, 0.25));

    let available_actions = vec![
        "move_forward",
        "move_backward",
        "move_left",
        "move_right",
        "jump_over",
        "climb_around",
        "wait_for_clear",
        "find_alternative",
        "assess_obstacle",
        "plan_detour",
        "retreat",
        "persist",
    ];

    let _q_estimator = NeuralQValueEstimator::new(0.1, 0.9);
    let _policy = ValueDrivenPolicy::new(0.1, 0.9, 0.2);

    let sankhara =
        ActiveInferenceSankharaSkandha::new(cwm, learning_engine, 6, available_actions, 0.9, 0.2);

    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"obstacle_scenario"));

    // Simulate dynamic obstacle scenario
    let mut position = (0, 0);
    let goal = (5, 5);
    let mut obstacles = vec![(2, 2), (3, 3)]; // Initial obstacles
    let mut path_taken = Vec::new();

    println!("üéØ Scenario: Navigate from (0,0) to (5,5) with dynamic obstacles");

    for step in 1..=15 {
        println!(
            "\nüîÑ Step {}: Position = {:?}, Goal = {:?}, Obstacles = {:?}",
            step, position, goal, obstacles
        );

        // Add dynamic obstacle every few steps
        if step % 4 == 0 && step < 12 {
            let new_obstacle = (step as i32, step as i32);
            if !obstacles.contains(&new_obstacle) {
                obstacles.push(new_obstacle);
                println!("   üöß New obstacle appeared at {:?}", new_obstacle);
            }
        }

        // Let the system plan
        sankhara.form_intent(&mut flow);

        let action = flow.sankhara.as_ref().map(|s| s.as_ref()).unwrap_or("wait");

        println!("   Planned action: {}", action);
        path_taken.push((position, action.to_string()));

        // Simulate action execution
        let new_position = match action {
            "move_forward" => (position.0, position.1 + 1),
            "move_backward" => (position.0, position.1 - 1),
            "move_left" => (position.0 - 1, position.1),
            "move_right" => (position.0 + 1, position.1),
            "jump_over" => {
                // Try to jump over obstacle
                let target = (position.0 + 1, position.1 + 1);
                if obstacles.contains(&target) {
                    println!("   ‚ùå Cannot jump over obstacle");
                    position
                } else {
                    target
                }
            }
            "climb_around" => {
                // Try to go around obstacle
                if obstacles.contains(&(position.0 + 1, position.1)) {
                    (position.0, position.1 + 1) // Go up instead
                } else {
                    (position.0 + 1, position.1)
                }
            }
            "wait_for_clear" => {
                println!("   ‚è≥ Waiting for obstacle to clear");
                position
            }
            "find_alternative" => {
                // Look for alternative path
                if position.0 < goal.0 {
                    (position.0 + 1, position.1)
                } else {
                    (position.0, position.1 + 1)
                }
            }
            _ => position,
        };

        // Check if new position is valid (not an obstacle)
        if !obstacles.contains(&new_position) {
            position = new_position;
            println!("   ‚úÖ Moved to {:?}", position);
        } else {
            println!("   ‚ùå Blocked by obstacle at {:?}", new_position);
        }

        // Check if goal reached
        if position == goal {
            println!("   üéâ GOAL REACHED!");
            break;
        }

        // Check for stuck condition
        if step > 10 && position == (0, 0) {
            println!("   ‚ö†Ô∏è  WARNING: Agent seems stuck at starting position");
        }
    }

    println!("\nüìä Dynamic Obstacle Planning Analysis:");
    println!("   Final position: {:?}", position);
    println!("   Goal: {:?}", goal);
    println!("   Path length: {}", path_taken.len());
    println!("   Success: {}", position == goal);

    // The agent should eventually reach the goal despite dynamic obstacles
    if position == goal {
        println!("‚úÖ Dynamic obstacle planning test passed!");
    } else {
        println!("‚ö†Ô∏è  Agent did not reach goal, but this may be acceptable for complex scenarios");
    }
}

/// Test planning efficiency and resource usage
#[cfg(test)]
#[test]
fn test_planning_efficiency() {
    println!("‚ö° Testing planning efficiency and resource usage");

    let config = GnnConfig::new(64, 128, 3);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.7, 0.3));

    let available_actions = vec![
        "action_1",
        "action_2",
        "action_3",
        "action_4",
        "action_5",
        "action_6",
        "action_7",
        "action_8",
        "action_9",
        "action_10",
    ];

    let _q_estimator = NeuralQValueEstimator::new(0.1, 0.9);
    let _policy = ValueDrivenPolicy::new(0.1, 0.9, 0.1);

    let sankhara =
        ActiveInferenceSankharaSkandha::new(cwm, learning_engine, 5, available_actions, 0.9, 0.1);

    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"efficiency_test"));

    // Measure planning time
    let start_time = std::time::Instant::now();

    for i in 1..=100 {
        sankhara.form_intent(&mut flow);

        if i % 20 == 0 {
            let elapsed = start_time.elapsed();
            println!("   Processed {} planning cycles in {:?}", i, elapsed);
        }
    }

    let total_time = start_time.elapsed();
    let avg_time_per_cycle = total_time / 100;

    println!("üìä Efficiency Results:");
    println!("   Total time: {:?}", total_time);
    println!("   Average time per cycle: {:?}", avg_time_per_cycle);
    println!(
        "   Cycles per second: {:.2}",
        100.0 / total_time.as_secs_f64()
    );

    // Performance assertions
    assert!(
        total_time.as_millis() < 1000,
        "Planning should complete within 1 second"
    );
    assert!(
        avg_time_per_cycle.as_millis() < 10,
        "Each cycle should take less than 10ms"
    );

    println!("‚úÖ Planning efficiency test passed!");
}
