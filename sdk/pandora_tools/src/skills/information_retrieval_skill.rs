use dashmap::DashMap;
use hnsw_rs::prelude::*;
use lancedb::Connection;
use serde_json::Value as CognitiveInput;
use std::collections::HashMap;
use std::time::{Duration, Instant};
use std::sync::{Arc, RwLock};
use thiserror::Error;

#[derive(Debug, Default, Clone)]
pub struct Confidence {
    pub score: f32,
    pub epistemic_uncertainty: f32,
    pub aleatoric_uncertainty: f32,
}

#[derive(Debug, Default, Clone)]
pub struct CognitiveOutput {
    pub content: String,
    pub confidence: Confidence,
    pub reasoning_trace: Vec<String>,
    pub documents: Vec<Document>,
}

#[derive(Debug, Error)]
pub enum RetrievalError {
    #[error("Lỗi kết nối LanceDB: {0}")]
    LanceDBConnection(String),
    #[error("Lỗi thực thi truy vấn LanceDB: {0}")]
    LanceDBQuery(String),
    #[error("Không tìm thấy document với ID: {0}")]
    DocumentNotFound(String),
    #[error("Input không hợp lệ cho việc tìm kiếm: {0}")]
    InvalidInput(String),
    #[error("Lỗi embedding: {0}")]
    EmbeddingError(String),
}

// ===== SEARCH MODES & CONFIGURATION =====

#[derive(Debug, Clone, PartialEq)]
pub enum SearchMode {
    UltraLight {
        max_memory_mb: usize,
        cache_only: bool,
        max_results: usize,
    },
    Balanced {
        max_memory_mb: usize,
        use_vector_search: bool,
        use_text_search: bool,
        hybrid_weight: f32,
    },
    Full {
        max_memory_mb: usize,
        use_all_tiers: bool,
        enable_kg_reasoning: bool,
        external_apis_enabled: bool,
    },
}

impl Default for SearchMode {
    fn default() -> Self {
        SearchMode::Balanced {
            max_memory_mb: 100,
            use_vector_search: true,
            use_text_search: true,
            hybrid_weight: 0.7,
        }
    }
}

#[derive(Debug, Clone)]
pub enum SearchStage {
    CacheLookup { max_age: Duration },
    VectorSearch { k: usize, threshold: f32 },
    TextSearch { k: usize, boost_factors: HashMap<String, f32> },
    KnowledgeGraphQuery { max_hops: usize },
    ExternalApiCall { apis: Vec<String>, timeout: Duration },
    ResultRanking { algorithm: RankingAlgorithm },
}

#[derive(Debug, Clone)]
pub enum RankingAlgorithm {
    CosineSimilarity,
    BM25,
    Hybrid,
    LearningToRank,
}

#[derive(Debug, Clone)]
pub enum FusionStrategy {
    ReciprocalRank,
    CombSum,
    CombMNZ,
    WeightedSum { weights: HashMap<String, f32> },
}

#[derive(Debug, Clone)]
pub struct SearchPipeline {
    pub stages: Vec<SearchStage>,
    pub early_exit_conditions: Vec<ExitCondition>,
    pub result_fusion: FusionStrategy,
}

#[derive(Debug, Clone)]
pub struct ExitCondition {
    pub stage_name: String,
    pub min_results: usize,
    pub max_latency: Duration,
    pub min_confidence: f32,
}

#[derive(Debug, Clone)]
pub struct QueryAnalyzer {
    pub query_types: HashMap<String, f32>,
    pub complexity_threshold: f32,
    pub intent_classifier: IntentClassifier,
}

#[derive(Debug, Clone)]
pub struct IntentClassifier {
    pub categories: Vec<String>,
    pub confidence_threshold: f32,
}

#[derive(Debug, Clone)]
pub struct ResultRanker {
    pub algorithm: RankingAlgorithm,
    pub learning_model: Option<String>,
    pub feature_weights: HashMap<String, f32>,
}

#[derive(Debug, Clone)]
pub struct FeedbackProcessor {
    pub feedback_history: Vec<FeedbackRecord>,
    pub learning_rate: f32,
    pub decay_factor: f32,
}

#[derive(Debug, Clone)]
pub struct FeedbackRecord {
    pub query: String,
    pub result_id: String,
    pub relevance_score: f32,
    pub timestamp: Instant,
}

/// Lưu trữ các sự thật có cấu trúc: (Subject, Vec<(Predicate, Object)>)
pub type KnowledgeGraph = DashMap<String, Vec<(String, String)>>;

/// Đại diện cho một tài liệu trong bộ nhớ
#[derive(Debug, Clone)]
pub struct Document {
    pub id: String,
    pub content: String,
    pub embedding: Vec<f32>,
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Engine tìm kiếm lũy tiến, kết hợp cache, vector DB và knowledge graph.
#[allow(dead_code)]
pub struct ProgressiveSemanticEngine {
    /// Cache HNSW cực nhanh trong bộ nhớ cho các truy vấn/kết quả phổ biến.
    hnsw_cache: Option<Hnsw<'static, f32, DistCosine>>,

    /// Map từ index của HNSW sang Document ID thực tế.
    cache_id_map: HashMap<usize, String>,

    /// Lõi lưu trữ chính, hỗ trợ hybrid search.
    lance_db_conn: Connection,

    /// Tên bảng trong LanceDB.
    lance_table_name: String,

    /// Kho tri thức có cấu trúc, an toàn luồng.
    knowledge_graph: KnowledgeGraph,

    /// Kích thước của vector embedding.
    embedding_dim: usize,

    /// Bộ nhớ trong (mock) để phục vụ test nhanh
    inmem_docs: Vec<Document>,

    // ===== NEW COMPONENTS FROM NEURAL SKILLS SPEC =====
    
    /// Search mode configuration
    search_mode: SearchMode,
    
    /// Search pipeline with stages
    search_pipeline: SearchPipeline,
    
    /// Query analyzer for intent classification
    query_analyzer: QueryAnalyzer,
    
    /// Result ranker with learning capabilities
    result_ranker: ResultRanker,
    
    /// Feedback processor for learning
    feedback_processor: FeedbackProcessor,
    
    /// Performance metrics
    performance_metrics: Arc<RwLock<HashMap<String, f32>>>,
    
    /// Quality threshold for results
    quality_threshold: f32,
    
    /// Maximum response time
    max_response_time: Duration,
}

impl ProgressiveSemanticEngine {
    /// Khởi tạo một engine tìm kiếm mới.
    pub async fn new(
        lance_db_path: &str,
        table_name: &str,
        embedding_dim: usize,
    ) -> Result<Self, RetrievalError> {
        let hnsw_cache = None;

        let conn = lancedb::connect(lance_db_path)
            .execute()
            .await
            .map_err(|e| RetrievalError::LanceDBConnection(e.to_string()))?;

        // TODO: Logic để tạo bảng LanceDB nếu chưa tồn tại.

        // Initialize new components
        let search_mode = SearchMode::default();
        let search_pipeline = Self::create_default_pipeline();
        let query_analyzer = QueryAnalyzer {
            query_types: HashMap::new(),
            complexity_threshold: 0.5,
            intent_classifier: IntentClassifier {
                categories: vec!["factual".to_string(), "procedural".to_string(), "conceptual".to_string()],
                confidence_threshold: 0.7,
            },
        };
        let result_ranker = ResultRanker {
            algorithm: RankingAlgorithm::Hybrid,
            learning_model: None,
            feature_weights: HashMap::new(),
        };
        let feedback_processor = FeedbackProcessor {
            feedback_history: Vec::new(),
            learning_rate: 0.1,
            decay_factor: 0.95,
        };

        Ok(Self {
            hnsw_cache,
            cache_id_map: HashMap::new(),
            lance_db_conn: conn,
            lance_table_name: table_name.to_string(),
            knowledge_graph: DashMap::new(),
            embedding_dim,
            inmem_docs: Vec::new(),
            search_mode,
            search_pipeline,
            query_analyzer,
            result_ranker,
            feedback_processor,
            performance_metrics: Arc::new(RwLock::new(HashMap::new())),
            quality_threshold: 0.7,
            max_response_time: Duration::from_millis(500),
        })
    }

    /// Tạo search pipeline mặc định
    fn create_default_pipeline() -> SearchPipeline {
        SearchPipeline {
            stages: vec![
                SearchStage::CacheLookup { max_age: Duration::from_secs(300) },
                SearchStage::VectorSearch { k: 10, threshold: 0.7 },
                SearchStage::TextSearch { k: 10, boost_factors: HashMap::new() },
                SearchStage::ResultRanking { algorithm: RankingAlgorithm::Hybrid },
            ],
            early_exit_conditions: vec![
                ExitCondition {
                    stage_name: "CacheLookup".to_string(),
                    min_results: 3,
                    max_latency: Duration::from_millis(50),
                    min_confidence: 0.8,
                },
            ],
            result_fusion: FusionStrategy::WeightedSum {
                weights: {
                    let mut w = HashMap::new();
                    w.insert("vector".to_string(), 0.7);
                    w.insert("text".to_string(), 0.3);
                    w
                },
            },
        }
    }

    /// Thêm một tài liệu mới vào hệ thống.
    pub async fn add_document(&mut self, doc: Document) -> Result<(), RetrievalError> {
        // TODO: ghi vào LanceDB; hiện tại thêm vào in-memory để test
        self.inmem_docs.push(doc);
        Ok(())
    }

    /// Tìm concept theo text (stub, sẽ được hiện thực sau)
    pub async fn search_by_text(
        &self,
        text: &str,
        top_k: usize,
    ) -> Result<Vec<Document>, RetrievalError> {
        let txt = text.to_lowercase();
        let mut matches: Vec<Document> = self
            .inmem_docs
            .iter()
            .filter(|d| d.content.to_lowercase().contains(&txt) || d.id.to_lowercase() == txt)
            .cloned()
            .collect();
        matches.truncate(top_k);
        Ok(matches)
    }

    /// Tìm concept theo vector (stub, sẽ được hiện thực sau)
    pub async fn search_by_vector(
        &self,
        vector: &[f32],
        top_k: usize,
    ) -> Result<Vec<Document>, RetrievalError> {
        // Cosine similarity với in-memory docs
        fn cosine(a: &[f32], b: &[f32]) -> f32 {
            let mut dot = 0.0f32;
            let mut na = 0.0f32;
            let mut nb = 0.0f32;
            for (x, y) in a.iter().zip(b.iter()) {
                dot += x * y;
                na += x * x;
                nb += y * y;
            }
            let denom = na.sqrt() * nb.sqrt();
            if denom > 0.0 {
                dot / denom
            } else {
                0.0
            }
        }
        let mut scored: Vec<(f32, &Document)> = self
            .inmem_docs
            .iter()
            .filter(|d| d.embedding.len() == vector.len())
            .map(|d| (cosine(vector, &d.embedding), d))
            .collect();
        scored.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));
        Ok(scored
            .into_iter()
            .take(top_k)
            .map(|(_, d)| d.clone())
            .collect())
    }

    /// Thực hiện tìm kiếm lũy tiến với search pipeline.
    pub async fn search(&self, input: &CognitiveInput) -> Result<CognitiveOutput, RetrievalError> {
        let start_time = Instant::now();
        let mut reasoning_trace = Vec::new();
        
        // 1. Parse input and extract query
        let query = self.extract_query_from_input(input)?;
        reasoning_trace.push(format!("Extracted query: '{}'", query));
        
        // 2. Analyze query complexity and intent
        let query_complexity = self.analyze_query_complexity(&query);
        let intent = self.classify_intent(&query);
        reasoning_trace.push(format!("Query complexity: {:.2}, Intent: {:?}", query_complexity, intent));
        
        // 3. Execute search pipeline
        let mut all_results = Vec::new();
        let mut stage_results = HashMap::new();
        
        for stage in &self.search_pipeline.stages {
            let stage_start = Instant::now();
            let stage_name = self.get_stage_name(stage);
            
            // Check early exit conditions
            if self.should_exit_early(&stage_name, &all_results, stage_start.elapsed()) {
                reasoning_trace.push(format!("Early exit at stage: {}", stage_name));
                break;
            }
            
            // Execute stage
            let stage_result = self.execute_stage(stage, &query, &intent).await?;
            let result_count = stage_result.len();
            stage_results.insert(stage_name.clone(), stage_result.clone());
            all_results.extend(stage_result);
            
            reasoning_trace.push(format!("Stage '{}' completed in {:?} with {} results", 
                stage_name, stage_start.elapsed(), result_count));
        }
        
        // 4. Fuse results using configured strategy
        let fused_results = self.fuse_results(&stage_results)?;
        reasoning_trace.push(format!("Fused {} results from {} stages", 
            fused_results.len(), stage_results.len()));
        
        // 5. Rank results
        let ranked_results = self.rank_results(fused_results, &query).await?;
        reasoning_trace.push(format!("Ranked {} results", ranked_results.len()));
        
        // 6. Calculate confidence and uncertainty
        let confidence = self.calculate_confidence(&ranked_results, &query);
        let epistemic_uncertainty = self.calculate_epistemic_uncertainty(&ranked_results);
        let aleatoric_uncertainty = self.calculate_aleatoric_uncertainty(&ranked_results);
        
        // 7. Update performance metrics
        self.update_performance_metrics(start_time.elapsed(), ranked_results.len() as f32);
        
        // 8. Return results
        Ok(CognitiveOutput {
            content: if ranked_results.is_empty() {
                "Không tìm thấy kết quả phù hợp".to_string()
            } else {
                ranked_results[0].content.clone()
            },
            confidence: Confidence {
                score: confidence,
                epistemic_uncertainty,
                aleatoric_uncertainty,
            },
            reasoning_trace,
            documents: ranked_results,
        })
    }

    /// Extract query from cognitive input
    fn extract_query_from_input(&self, input: &CognitiveInput) -> Result<String, RetrievalError> {
        match input {
            CognitiveInput::String(s) => Ok(s.clone()),
            CognitiveInput::Object(obj) => {
                if let Some(query) = obj.get("query").and_then(|v| v.as_str()) {
                    Ok(query.to_string())
                } else if let Some(text) = obj.get("text").and_then(|v| v.as_str()) {
                    Ok(text.to_string())
                } else {
                    Err(RetrievalError::InvalidInput("No query or text field found".to_string()))
                }
            }
            _ => Err(RetrievalError::InvalidInput("Unsupported input type".to_string()))
        }
    }

    /// Analyze query complexity
    fn analyze_query_complexity(&self, query: &str) -> f32 {
        let mut complexity = 0.0;
        
        // Length factor
        complexity += (query.len() as f32 / 100.0).min(1.0) * 0.3;
        
        // Word count factor
        let word_count = query.split_whitespace().count();
        complexity += (word_count as f32 / 20.0).min(1.0) * 0.3;
        
        // Question mark factor
        if query.contains('?') {
            complexity += 0.2;
        }
        
        // Special characters factor
        let special_chars = query.chars().filter(|c| !c.is_alphanumeric() && !c.is_whitespace()).count();
        complexity += (special_chars as f32 / 10.0).min(1.0) * 0.2;
        
        complexity.min(1.0)
    }

    /// Classify query intent
    fn classify_intent(&self, query: &str) -> String {
        let query_lower = query.to_lowercase();
        
        if query_lower.contains("how") || query_lower.contains("what") || query_lower.contains("why") {
            "factual".to_string()
        } else if query_lower.contains("how to") || query_lower.contains("step") || query_lower.contains("process") {
            "procedural".to_string()
        } else if query_lower.contains("what is") || query_lower.contains("define") || query_lower.contains("concept") {
            "conceptual".to_string()
        } else {
            "factual".to_string()
        }
    }

    /// Get stage name for logging
    fn get_stage_name(&self, stage: &SearchStage) -> String {
        match stage {
            SearchStage::CacheLookup { .. } => "CacheLookup".to_string(),
            SearchStage::VectorSearch { .. } => "VectorSearch".to_string(),
            SearchStage::TextSearch { .. } => "TextSearch".to_string(),
            SearchStage::KnowledgeGraphQuery { .. } => "KnowledgeGraphQuery".to_string(),
            SearchStage::ExternalApiCall { .. } => "ExternalApiCall".to_string(),
            SearchStage::ResultRanking { .. } => "ResultRanking".to_string(),
        }
    }

    /// Check if should exit early based on conditions
    fn should_exit_early(&self, stage_name: &str, results: &[Document], elapsed: Duration) -> bool {
        for condition in &self.search_pipeline.early_exit_conditions {
            if condition.stage_name == stage_name {
                return results.len() >= condition.min_results 
                    && elapsed <= condition.max_latency
                    && self.calculate_average_confidence(results) >= condition.min_confidence;
            }
        }
        false
    }

    /// Execute a search stage
    async fn execute_stage(&self, stage: &SearchStage, query: &str, _intent: &str) -> Result<Vec<Document>, RetrievalError> {
        match stage {
            SearchStage::CacheLookup { .. } => {
                // For now, return empty results (cache not implemented)
                Ok(Vec::new())
            }
            SearchStage::VectorSearch { k, threshold: _ } => {
                // Mock vector search - in real implementation, would use actual embeddings
                let mock_vector = vec![0.1; self.embedding_dim];
                self.search_by_vector(&mock_vector, *k).await
            }
            SearchStage::TextSearch { k, boost_factors: _ } => {
                self.search_by_text(query, *k).await
            }
            SearchStage::KnowledgeGraphQuery { max_hops: _ } => {
                // Mock knowledge graph query
                Ok(Vec::new())
            }
            SearchStage::ExternalApiCall { apis: _, timeout: _ } => {
                // Mock external API call
                Ok(Vec::new())
            }
            SearchStage::ResultRanking { algorithm: _ } => {
                // This stage is handled in the main search method
                Ok(Vec::new())
            }
        }
    }

    /// Fuse results from different stages
    fn fuse_results(&self, stage_results: &HashMap<String, Vec<Document>>) -> Result<Vec<Document>, RetrievalError> {
        match &self.search_pipeline.result_fusion {
            FusionStrategy::WeightedSum { weights } => {
                let mut fused = Vec::new();
                for (stage_name, results) in stage_results {
                    if let Some(_weight) = weights.get(stage_name) {
                        for doc in results.clone() {
                            // Apply weight to confidence (if we had confidence scores)
                            fused.push(doc);
                        }
                    }
                }
                Ok(fused)
            }
            _ => {
                // Default: just combine all results
                let mut all_results = Vec::new();
                for results in stage_results.values() {
                    all_results.extend(results.clone());
                }
                Ok(all_results)
            }
        }
    }

    /// Rank results using configured algorithm
    async fn rank_results(&self, mut results: Vec<Document>, _query: &str) -> Result<Vec<Document>, RetrievalError> {
        // Simple ranking by content length (mock implementation)
        results.sort_by(|a, b| b.content.len().cmp(&a.content.len()));
        Ok(results)
    }

    /// Calculate overall confidence
    fn calculate_confidence(&self, results: &[Document], _query: &str) -> f32 {
        if results.is_empty() {
            0.0
        } else {
            // Mock confidence calculation
            0.8
        }
    }

    /// Calculate epistemic uncertainty
    fn calculate_epistemic_uncertainty(&self, results: &[Document]) -> f32 {
        if results.is_empty() {
            1.0
        } else {
            // Mock epistemic uncertainty
            0.2
        }
    }

    /// Calculate aleatoric uncertainty
    fn calculate_aleatoric_uncertainty(&self, _results: &[Document]) -> f32 {
        // Mock aleatoric uncertainty
        0.1
    }

    /// Calculate average confidence of results
    fn calculate_average_confidence(&self, results: &[Document]) -> f32 {
        if results.is_empty() {
            0.0
        } else {
            // Mock average confidence
            0.8
        }
    }

    /// Update performance metrics
    fn update_performance_metrics(&self, duration: Duration, result_count: f32) {
        let mut metrics = self.performance_metrics.write().unwrap();
        metrics.insert("avg_response_time_ms".to_string(), duration.as_millis() as f32);
        metrics.insert("avg_result_count".to_string(), result_count);
        
        let current_total = metrics.get("total_queries").copied().unwrap_or(0.0);
        metrics.insert("total_queries".to_string(), current_total + 1.0);
    }
}
